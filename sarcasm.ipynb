{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74193f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4912ccdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import AzureOpenAI\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "client = AzureOpenAI(\n",
    "    azure_endpoint=os.environ.get(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    api_key=os.environ.get(\"AZURE_OPENAI_API_KEY\"),\n",
    "    api_version=\"2025-04-01-preview\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb62b687",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVELOPER_PROMPT = \"Clippy is a factual chatbot that is also sarcastic.\"\n",
    "\n",
    "BASE_MODELS = [\n",
    "    \"o4-mini\",\n",
    "    \"gpt-4.1\",\n",
    "    \"gpt-4.1-mini\",\n",
    "    \"gpt-4.1-nano\",\n",
    "    \"gpt-4o\",\n",
    "    \"gpt-4o-mini\"\n",
    "]\n",
    "\n",
    "GRADER_MODEL = \"o3\"\n",
    "GRADER_PROMPT = \"\"\"\n",
    "You are an expert in sarcasm detection, while also being a good fact checker.\n",
    "\n",
    "You'll be given a conversation in the form of a question and answer. \n",
    "\n",
    "## Scoring Criteria\n",
    "Judge the answer using two metrics.\n",
    "\n",
    "### Metric 1: How sarcastic is the response?\n",
    "On a scale from 1 to 10 inclusive with 1 being no sarcasm detected and 10 being\n",
    "the most sarcastic response ever. Assign an integer value within that range based\n",
    "on your assessment.\n",
    "\n",
    "### Metric 2: How accurate is the response?\n",
    "Assign a 1 if the response is factually correct. Assign a 0 for this metric if it's\n",
    "incorrect or contains innacuracies.\n",
    "\n",
    "### Final Score\n",
    "The final score you must decide should be based on a weighted blend of Metric 1 and\n",
    "Metric 2 using the formula: `(Metric 1) * (Metric 2)`\n",
    "\n",
    "This means that if Metric 2 is zero, the final score must be zero.\n",
    "\n",
    "## Response Structure\n",
    "Your response must be in a JSON format that can be loaded by Python's json.loads()\n",
    "function. It should resemble the following:\n",
    "\n",
    "```\n",
    "{\n",
    "  \"steps\": [\n",
    "    { \n",
    "      \"description\": <one sentence describing your reasoning for Metric 1>\", \n",
    "      \"result\": <string representation of Metric 1's score> \n",
    "    },\n",
    "    { \n",
    "      \"description\": <one sentence describing your reasoning for Metric 1>\", \n",
    "      \"result\": <string representation of Metric 1's score> \n",
    "    }\n",
    "  ],\n",
    "  \"result\": <floating point value of the Final Score>\n",
    "}\n",
    "\n",
    "## General Guidance\n",
    "The questions should be simple factual questions. Deep research is not required.\n",
    "```\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6b9289",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_file = None\n",
    "with open(\"./sarcasm-qa-sample.jsonl\", \"rb\") as f:\n",
    "    eval_file = client.files.create(purpose=\"evals\", file=f)\n",
    "    client.files.wait_for_processing(eval_file.id)\n",
    "    print(f\"Created eval file: {eval_file.id}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a6cbf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's define an evaluation using some static input from question/answer pairs.\n",
    "\n",
    "# The entire user prompt is data driven from the file. No generation is done using\n",
    "# a model, just simple string substitution using this pattern.\n",
    "USER_PROMPT = \"\"\"\n",
    "Q: {{item.question}}\n",
    "A: {{item.answer}}\n",
    "\"\"\"\n",
    "INPUT = [\n",
    "    {\n",
    "        \"type\": \"message\",\n",
    "        \"role\": \"developer\",\n",
    "        \"content\": { \"type\": \"input_text\", \"text\": GRADER_PROMPT }\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"message\",\n",
    "        \"role\": \"user\",\n",
    "        \"content\": { \"type\": \"input_text\", \"text\": USER_PROMPT }\n",
    "    }\n",
    "]\n",
    "\n",
    "# We need to describe what our evaluation dataset looks like.\n",
    "SCHEMA = {\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "        \"question\": { \"type\": \"string\" },\n",
    "        \"answer\": { \"type\": \"string\" },\n",
    "    }\n",
    "}\n",
    "DATA_SOURCE = {\n",
    "    \"item_schema\": SCHEMA,\n",
    "    \"include_sample_schema\": False,\n",
    "    \"type\": \"custom\",\n",
    "}\n",
    "\n",
    "# Lastly, we define test criteria that combines the above.\n",
    "TESTING_CRITERIA = {\n",
    "    \"name\": \"Auto Sarcasm Grader\",\n",
    "    \"type\": \"score_model\",\n",
    "    \"model\": GRADER_MODEL,\n",
    "    \"input\": INPUT,\n",
    "    \"range\": [1.0, 10.0],\n",
    "    \"pass_threshold\": 4.0,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0a7eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "suffix = str(uuid.uuid4()).split(\"-\")[0]\n",
    "\n",
    "sample_eval = client.evals.create(\n",
    "    name=f\"sarcasm-sampledata-{suffix}\",\n",
    "    data_source_config=DATA_SOURCE,\n",
    "    testing_criteria=[TESTING_CRITERIA],\n",
    ")\n",
    "\n",
    "print(f\"Submitted evaluation {sample_eval.id}\")\n",
    "\n",
    "# Start a Run\n",
    "# Need to tell it where our sample data lives.\n",
    "RUN_DATA_SOURCE = {\n",
    "    \"type\": \"jsonl\",\n",
    "    \"source\": { \"type\": \"file_id\", \"id\": eval_file.id }\n",
    "}\n",
    "sample_run = client.evals.runs.create(\n",
    "    name=f\"sample-data-{GRADER_MODEL}\",\n",
    "    eval_id=sample_eval.id,\n",
    "    data_source=RUN_DATA_SOURCE,\n",
    ")\n",
    "print(f\"Submitted run {sample_run.id} to eval {sample_eval.id}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080dd428",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wait for the evals to complete.\n",
    "from IPython.display import clear_output\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "sample_run = client.evals.runs.retrieve(eval_id=sample_eval.id, run_id=sample_run.id)\n",
    "while sample_run.status not in [\"completed\", \"failed\"]:\n",
    "    time.sleep(5)\n",
    "    clear_output(wait=True)\n",
    "\n",
    "    sample_run = client.evals.runs.retrieve(eval_id=sample_eval.id, run_id=sample_run.id)\n",
    "    \n",
    "    now = time.time()\n",
    "    mins, secs = int((time.time() - start_time) // 60), int((time.time() - start_time) % 60)\n",
    "    print(f\"Elapsed time: {mins} minutes {secs} seconds\")\n",
    "\n",
    "print(f\"Run {sample_run.id}: {sample_run.status}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd72521a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display an evaluation summary\n",
    "from scripts.eval_utils import display_evaluation_summary\n",
    "\n",
    "display_evaluation_summary(client, [sample_eval.id], x_range=(0, 10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73f399e",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_file = None\n",
    "with open(\"./sarcasm-qa.jsonl\", \"rb\") as f:\n",
    "    qa_file = client.files.create(purpose=\"evals\", file=f)\n",
    "    client.files.wait_for_processing(qa_file.id)\n",
    "    print(f\"Created eval file: {qa_file.id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69774305",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's test some base models!\n",
    "\n",
    "# GRADER_PROMPT is re-used!\n",
    "# We *do* create a new USER_PROMPT as now we generate the Answer via a model\n",
    "# under test. Notice how we reference {{sample.output_text}}.\n",
    "USER_PROMPT = \"\"\"\n",
    "Q: {{item.question}}\n",
    "A: {{sample.output_text}}\n",
    "\"\"\"\n",
    "INPUT = [\n",
    "    {\n",
    "        \"type\": \"message\",\n",
    "        \"role\": \"developer\",\n",
    "        \"content\": { \"type\": \"input_text\", \"text\": GRADER_PROMPT }\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"message\",\n",
    "        \"role\": \"user\",\n",
    "        \"content\": { \"type\": \"input_text\", \"text\": USER_PROMPT }\n",
    "    }\n",
    "]\n",
    "SCHEMA = {\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "        \"question\": { \"type\": \"string\" },\n",
    "        \"answer\": { \"type\": \"string\" },\n",
    "    },\n",
    "}\n",
    "DATA_SOURCE = {\n",
    "    \"item_schema\": SCHEMA, \n",
    "    \"include_sample_schema\": True, # Note this change! Needed for data gen.\n",
    "    \"type\": \"custom\"\n",
    "}\n",
    "TESTING_CRITERIA = {\n",
    "    \"name\": \"Auto Sarcasm Grader\",\n",
    "    \"type\": \"score_model\",\n",
    "    \"model\": GRADER_MODEL,\n",
    "    \"input\": INPUT,\n",
    "    \"range\": [1.0, 10.0],\n",
    "    \"pass_threshold\": 4.0,\n",
    "}\n",
    "\n",
    "eval = client.evals.create(\n",
    "    name=f\"sacarsm-evaluation-{suffix}\",\n",
    "    data_source_config=DATA_SOURCE,\n",
    "    testing_criteria=[TESTING_CRITERIA]\n",
    ")\n",
    "print(f\"Created eval {eval.id}\")\n",
    "\n",
    "runs = []\n",
    "for model in BASE_MODELS:\n",
    "    DATA_SOURCE = {\n",
    "        \"type\": \"completions\",\n",
    "        \"model\": model,\n",
    "        \"source\": { \"type\": \"file_id\", \"id\": qa_file.id },\n",
    "        \"input_messages\": {\n",
    "            \"type\": \"template\",\n",
    "            \"template\": [\n",
    "                { \n",
    "                    \"type\": \"message\", \n",
    "                    \"role\": \"developer\", \n",
    "                    \"content\": { \"type\": \"input_text\", \"text\": DEVELOPER_PROMPT },\n",
    "                },\n",
    "                { \n",
    "                    \"type\": \"message\", \n",
    "                    \"role\": \"user\", \n",
    "                    \"content\": { \"type\": \"input_text\", \"text\": \"{{item.question}}\" },\n",
    "                },\n",
    "            ],\n",
    "        },\n",
    "        \"sampling_params\": { \"max_completions_tokens\": 20_000 } if model.startswith(\"o\") else { \"max_completions_tokens\": 100 }, # XXX\n",
    "    }\n",
    "    run = client.evals.runs.create(\n",
    "        name=f\"{model}-{suffix}\", \n",
    "        eval_id=eval.id,\n",
    "        data_source=DATA_SOURCE, \n",
    "    )\n",
    "    print(f\"Created run {run.id} for eval {eval.id}\")\n",
    "    runs.append(run)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d83696a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wait for the Runs to complete.\n",
    "from IPython.display import clear_output\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "while any([r.status != \"completed\" for r in runs]):\n",
    "    time.sleep(10)\n",
    "    clear_output(wait=True)\n",
    "\n",
    "    for i in range(len(runs)):\n",
    "        runs[i] = client.evals.runs.retrieve(eval_id=eval.id, run_id=runs[i].id)\n",
    "        print(f\"Run {runs[i].name}: {runs[i].status}\")\n",
    "    \n",
    "    print(\"Elapsed time: {} minutes {} seconds\".format(int((time.time() - start_time) // 60), int((time.time() - start_time) % 60)))\n",
    "\n",
    "print(f\"All {len(runs)} runs completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5acc0dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the display_evaluation_summary function from the eval_utils script\n",
    "from scripts.eval_utils import display_evaluation_summary\n",
    "\n",
    "# Display the evaluation summary\n",
    "display_evaluation_summary(client, [eval.id], x_range=(0, 10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d0bd09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Let's try making 4.1-nano be a little better. 4.1 seems to be the best here, so let's have it be the teacher.\n",
    "TEACHER_MODEL = \"gpt-4.1\"\n",
    "\n",
    "# We'll reuse the same QA data set, but we'll need to reformat it into Chat Completions format.\n",
    "chats = []\n",
    "with open(\"./sarcasm-qa.jsonl\", \"r\") as f:\n",
    "    for line in f.readlines():\n",
    "        qa = json.loads(line)\n",
    "        chats.append([\n",
    "            { \"role\": \"system\", \"content\": DEVELOPER_PROMPT },\n",
    "            { \"role\": \"user\", \"content\": qa[\"question\"] },\n",
    "        ])\n",
    "\n",
    "# We'll keep this simple and serialized. If you wanted to do this at scale, you should parallelize this step\n",
    "# so you aren't generating one chat completion at a time ;)\n",
    "errors = []\n",
    "replies = []\n",
    "print(f\"🧪 Attempting to distill {len(chats)} prompts.\")\n",
    "for idx, chat in enumerate(tqdm(chats)):\n",
    "    metadata = {\n",
    "        \"dataset\": \"sarcasm\",\n",
    "        \"teacher\": TEACHER_MODEL,\n",
    "        \"index\": str(idx),\n",
    "        \"suffix\": suffix,\n",
    "    }\n",
    "    try:\n",
    "        reply = client.chat.completions.create(\n",
    "            model=TEACHER_MODEL,\n",
    "            messages=chat,\n",
    "            store=True,\n",
    "            metadata=metadata,\n",
    "            max_completion_tokens=100\n",
    "        )\n",
    "        messages = chat + [{ \"role\": \"assistant\", \"content\": reply.choices[0].message.content.strip() }]\n",
    "        replies.append({ \"messages\": messages })\n",
    "    except Exception as e:\n",
    "        errors.append(f\"⚠️ prompt {idx} failed: {e}\")\n",
    "\n",
    "print(f\"🧪 Distilled {len(replies)} out of a possible {len(chats)} prompts.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3e5874",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we need to create our SFT training and validation set. We'll do a pretty simple split here.\n",
    "dataset = {\n",
    "    \"training\": replies[:80],\n",
    "    \"validation\": replies[80:],\n",
    "}\n",
    "\n",
    "# Now we create files via the Files API. To keep it simple, we write them to disk first and then\n",
    "# we use the SDK to upload.\n",
    "files = {\n",
    "    \"training\": None,\n",
    "    \"validation\": None,\n",
    "}\n",
    "for name, chats in dataset.items():\n",
    "    with open(f\"sarcasm-{name}.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
    "        for chat in chats:\n",
    "            json.dump(chat, f)\n",
    "            f.write(\"\\n\")\n",
    "    with open(f\"sarcasm-{name}.jsonl\", \"rb\") as f:\n",
    "        file = client.files.create(file=f, purpose=\"fine-tune\")\n",
    "        file = client.files.wait_for_processing(file.id)\n",
    "        files[name] = file\n",
    "    print(f\"Created {name} file:\\n{file.model_dump_json(indent=2)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e8b7a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we submit our fine-tuning job!\n",
    "STUDENT_MODEL = \"gpt-4.1-mini-2025-04-14\"\n",
    "SUFFIX = f\"{TEACHER_MODEL}-sarcasm\".replace(\".\", \"\") # '.' is a reserved character 😜\n",
    "\n",
    "job = client.fine_tuning.jobs.create(\n",
    "    model=STUDENT_MODEL,\n",
    "    suffix=SUFFIX,\n",
    "    training_file=files[\"training\"].id,\n",
    "    validation_file=files[\"validation\"].id,\n",
    "    extra_body={ \"trainingType\": \"globalstandard\" },\n",
    ")\n",
    "print(f\"Created fine-tuning job:\\n{job.to_json(indent=2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88aa266f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wait for our FT job to complete.\n",
    "from IPython.display import clear_output\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "status = job.status\n",
    "while status not in [\"succeeded\", \"failed\", \"cancelled\"]:\n",
    "    time.sleep(15)\n",
    "    job = client.fine_tuning.jobs.retrieve(job.id)\n",
    "    status = job.status\n",
    "    clear_output(wait=True)\n",
    "    print(f\"Job {job.id}: {status}\")\n",
    "    print(\"Elapsed time: {} minutes {} seconds\".format(int((time.time() - start_time) // 60), int((time.time() - start_time) % 60)))\n",
    "\n",
    "if status == \"succeeded\":\n",
    "    print(f\"Fine-tuning finished!\")\n",
    "else:\n",
    "    raise RuntimeError(f\"Fine-tuning job did not complete successfully (status={status})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dee6381",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we need to deploy the new model candidate. We'll use Developer Tier to keep\n",
    "# costs to just per-token!\n",
    "import os\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.mgmt.cognitiveservices import CognitiveServicesManagementClient\n",
    "\n",
    "# The OpenAI client cannot control the Azure OpenAI control plane, so we need a\n",
    "# different client specific to Azure Cognitive Services.\n",
    "cogsvc_client = CognitiveServicesManagementClient(\n",
    "    credential=DefaultAzureCredential(),\n",
    "    subscription_id=os.environ.get(\"AZURE_SUBSCRIPTION_ID\")\n",
    ")\n",
    "\n",
    "# Define our Deployment. Note the use of SKU for specificy capacity and\n",
    "# the name of the deployment tier.\n",
    "DEPLOYMENT_NAME = f\"sarcasm-{TEACHER_MODEL.replace(\".\", \"\")}-distilled\"\n",
    "DEPLOYMENT = {\n",
    "    \"properties\": {\n",
    "        \"model\": { \n",
    "            \"format\": \"OpenAI\", \n",
    "            \"name\": job.fine_tuned_model, \n",
    "            \"version\": \"1\" \n",
    "        },\n",
    "    },\n",
    "    \"sku\": { \n",
    "        \"capacity\": 250, \n",
    "        \"name\": \"DeveloperTier\" \n",
    "    },\n",
    "}\n",
    "\n",
    "# Submit the request for provisioning. This may take a few minutes, so we\n",
    "# poll for updates. If it already exists, this should return quickly.\n",
    "deployment = cogsvc_client.deployments.begin_create_or_update(\n",
    "    resource_group_name=os.environ.get(\"AZURE_RESOURCE_GROUP\"),\n",
    "    account_name=os.environ.get(\"AZURE_AOAI_ACCOUNT\"),\n",
    "    deployment_name=DEPLOYMENT_NAME,\n",
    "    deployment=DEPLOYMENT,\n",
    ")\n",
    "print(f\"Submitted deployment {deployment}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c4168a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wait for our deployment to finish provisioning.\n",
    "from IPython.display import clear_output\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "status = deployment.status()\n",
    "print(f\"Provisioning {DEPLOYMENT_NAME}: {status}\")\n",
    "\n",
    "while status not in [\"Succeeded\", \"Failed\"]:\n",
    "    deployment.wait(5)\n",
    "    status = deployment.status()\n",
    "    clear_output(wait=True)\n",
    "    print(f\"Provisioning {DEPLOYMENT_NAME}: {status}\")\n",
    "    print(\"Elapsed time: {} minutes {} seconds\".format(int((time.time() - start_time) // 60), int((time.time() - start_time) % 60)))\n",
    "\n",
    "print(f\"Provisioning finished!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b6d115d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we add a new Run to our existing Evaluation so we can easily compare our\n",
    "# distilled model against the base models to see if we successfully improved\n",
    "# our student model.\n",
    "DATA_SOURCE = {\n",
    "    \"type\": \"completions\",\n",
    "    \"model\": DEPLOYMENT_NAME,\n",
    "    \"source\": { \"type\": \"file_id\", \"id\": qa_file.id },\n",
    "    \"input_messages\": {\n",
    "        \"type\": \"template\",\n",
    "        \"template\": [\n",
    "            { \n",
    "                \"type\": \"message\", \n",
    "                \"role\": \"developer\", \n",
    "                \"content\": { \"type\": \"input_text\", \"text\": DEVELOPER_PROMPT },\n",
    "            },\n",
    "            { \n",
    "                \"type\": \"message\", \n",
    "                \"role\": \"user\", \n",
    "                \"content\": { \"type\": \"input_text\", \"text\": \"{{item.question}}\" },\n",
    "            },\n",
    "        ],\n",
    "    },\n",
    "    \"sampling_params\": {\n",
    "        \"max_completions_tokens\": 250, # we SFT'd a non-reasoning model, so keep this tight\n",
    "        # the above should be `max_completion_tokens`...note the singular completion!\n",
    "    }\n",
    "}\n",
    "\n",
    "# Submit the Run.\n",
    "run = client.evals.runs.create(\n",
    "    name=f\"AutoGrader-{job.fine_tuned_model}-{suffix}\", eval_id=eval.id, data_source=DATA_SOURCE,\n",
    ")\n",
    "print(f\"Created new run:\\n{run.to_json(indent=2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67769d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wait for the new Run to complete.\n",
    "from IPython.display import clear_output\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "while run.status not in [\"completed\", \"failed\"]:\n",
    "    time.sleep(10)\n",
    "    clear_output(wait=True)\n",
    "    run = client.evals.runs.retrieve(eval_id=eval.id, run_id=run.id)\n",
    "    print(f\"Run {run.id}: {run.status}\")\n",
    "    \n",
    "    print(\"Elapsed time: {} minutes {} seconds\".format(int((time.time() - start_time) // 60), int((time.time() - start_time) % 60)))\n",
    "\n",
    "print(f\"Run {run.id} completed!\")\n",
    "print(run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae06ec92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the display_evaluation_summary function from the eval_utils script\n",
    "from scripts.eval_utils import display_evaluation_summary\n",
    "\n",
    "# Display the evaluation summary\n",
    "display_evaluation_summary(client, [eval.id])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
